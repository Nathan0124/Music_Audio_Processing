{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 3 â€” (15 points)\n",
    "======\n",
    "### What to hand in\n",
    "You are to submit the following things for this homework:\n",
    "1. A Jupyter notebook containing all code and output (figures and audio). I should be able to evaluate the file to reproduce all output. \n",
    "1. Any other data that we tell you to save to a file (e.g. audio files).\n",
    "\n",
    "### How to hand it in\n",
    "To submit your lab:\n",
    "1. Compress all of the files specified into a .zip file. \n",
    "1. Name the file in the following manner, firstname_lastname_hw1.zip. For example, Bryan_Pardo_hw1.zip. \n",
    "1. Submit this .zip file via Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this code block 1st, to import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Profession\\Development\\Miniconda2\\envs\\eecs352\\lib\\site-packages\\librosa\\core\\audio.py:37: UserWarning: Could not import scikits.samplerate. Falling back to scipy.signal\n",
      "  warnings.warn('Could not import scikits.samplerate. '\n"
     ]
    }
   ],
   "source": [
    "# This line is a convenience to import most packages you'll need. You may need to import others (eg random and cmath)\n",
    "import IPython, numpy as np, scipy as sp, matplotlib.pyplot as plt, matplotlib, sklearn, librosa, cmath,math\n",
    "from IPython.display import Audio\n",
    " \n",
    "# This line makes sure your plots happen IN the webpage you're building, instead of in separate windows.\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helpful Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import hann\n",
    "\n",
    "def wavwrite(filepath, data, sr, norm=True, dtype='int16',):\n",
    "    '''\n",
    "    Write wave file using scipy.io.wavefile.write, converting from a float (-1.0 : 1.0) numpy array to an integer array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The path of the output .wav file\n",
    "    data : np.array\n",
    "        The float-type audio array\n",
    "    sr : int\n",
    "        The sampling rate\n",
    "    norm : bool\n",
    "        If True, normalize the audio to -1.0 to 1.0 before converting integer\n",
    "    dtype : str\n",
    "        The output type. Typically leave this at the default of 'int16'.\n",
    "    '''\n",
    "    if norm:\n",
    "        data /= np.max(np.abs(data))\n",
    "    data = data * np.iinfo(dtype).max\n",
    "    data = data.astype(dtype)\n",
    "    sp.io.wavfile.write(filepath, sr, data)\n",
    "    \n",
    "#-----------------------------------------------------------\n",
    "# Here's an stft implementation. Yes, it doesn't bother to get the last little bit of audio.\n",
    "   \n",
    "def stft(signal, window_size, hop_size, window_type = 'hann'):\n",
    "    \"\"\"\n",
    "    Computes the short term fourier transform of a 1-D numpy array, where the array \n",
    "    is windowed into a set of subarrays, each of length window_size. The distance between\n",
    "    window centers (in samples) is given by hop_size. The type of window applied is\n",
    "    determined by window_type. This returns a 2-D numpy array where the ith column\n",
    "    is the FFT of the ith window. Each column contains an array of complex values.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    signal: The 1-d (complex or real) numpy array containing the signal\n",
    "    window_size: an integer scalar specifying the number of samples in a window\n",
    "    hop_size: an integer specifying the number of samples between the start of adjacent windows\n",
    "    window_type: a string specifying one of two \"hann\" or \"rectangular\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a 2D numpy array of complex numbers where the array column is the FFT of the ith window,\n",
    "    and the jth element in the ith column is the jth frequency of analysis.\n",
    "    \"\"\"\n",
    "    # figure out how many hops\n",
    "    length_to_cover_with_hops = len(signal) - window_size;\n",
    "    assert (length_to_cover_with_hops >= 0), \"window_size cannot be longer than the signal to be windowed\"\n",
    "    num_hops = 1 + length_to_cover_with_hops/hop_size;\n",
    "    \n",
    "    # make our window function\n",
    "    if window_type == 'hann':\n",
    "        window = hann(window_size)\n",
    "    elif window_type=='rectangular':\n",
    "        window = np.ones(window_size)\n",
    "    else:\n",
    "        raise Exception('Invalid window type. Must be \"hann\" or \"rectangular\".')\n",
    "    \n",
    "    stft = [0]*num_hops\n",
    "    # fill the array with values \n",
    "    for hop in range(num_hops):\n",
    "        start = hop*hop_size\n",
    "        end = start + window_size\n",
    "        unwindowed_sound = signal[start:end]\n",
    "        windowed_sound =  unwindowed_sound * window\n",
    "        stft[hop]= fft(windowed_sound, window_size) \n",
    "    return np.array(stft)\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# here's a spectrogram implementation\n",
    "\n",
    "def spectrogram(signal, window_size, hop_size, sample_rate, window_type = 'hann', display = True ):\n",
    "    \"\"\"\n",
    "    Computes the short term fourier transform of a 1-D numpy array, where the array \n",
    "    is windowed into a set of subarrays, each of length window_size. The distance between\n",
    "    window centers (in samples) is given by hop_size. The type of window applied is\n",
    "    determined by window_type. This creates a 2-D numpy array where the ith column\n",
    "    is the FFT of the ith window. Each column contains an array of complex values.\n",
    "    It then creates a magnitude spectrogram of the signal and plots it on the screen.\n",
    "    Here, the vertical dimension is frequency (in Hz), the horizontal dimension is time\n",
    "    (in seconds), brightness corresponds to amplitude (in dB). Only frequencies up to\n",
    "    the Nyquist rate are displayed.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    signal: The 1-d (complex or real) numpy array containing the signal\n",
    "    window_size: an integer scalar specifying the number of samples in a window\n",
    "    hop_size: an integer specifying the number of samples between the start of adjacent windows\n",
    "    sample_rate: an integer giving the sample rate of the input signal, in Hz\n",
    "    window_type: a string specifying one of two \"hann\" or \"rectangular\"\n",
    "    display: a bool. If set to True, it plots the spectrogram. Else it does not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    an output tuple with 3 items\n",
    "    \n",
    "    sgram:  a 2-D numpy array of real-valued numbers that contains the magnitude spectrogram\n",
    "           sgram[t,f] is the magnitude at time t and frequency f. This only contains values\n",
    "           up to the nyquist frequency\n",
    "    times: a 1-D numpy array of non-negative real-values that gives the times,  \n",
    "           times[t] gives the start time of the tth window in seconds\n",
    "    freqs: a 1-D numpy array  of non-negative real values. freqs[f] gives the fth\n",
    "           frequency of analysis in Hz, up to the nyquist frequency\n",
    "           \n",
    "    Calling Example\n",
    "    ---------------\n",
    "    sgram,times,freqs = spectrogram(signal, window_size, hop_size, sample_rate )\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    # get the stft\n",
    "    X = stft(signal, window_size, hop_size, window_type)\n",
    "\n",
    "    # figure out what my window start times are\n",
    "    times = np.arange(len(X))\n",
    "    hop_in_secs = hop_size/(1.0 * sample_rate)\n",
    "    times = times * hop_in_secs\n",
    "    \n",
    "    # chop off everything above the nyquist rate & rotate for display    \n",
    "    X = np.rot90(X,3)\n",
    "    sgram = np.array(X[0:len(X)/2])\n",
    "\n",
    "    # figure out what my frequencies of analysis are..\n",
    "    analysis_f0 = sample_rate * 1.0 /window_size\n",
    "    freqs = np.arange(len(sgram)) * analysis_f0\n",
    "    \n",
    "    # turn the complex values into magnitudes and putit into a log scale \n",
    "    sgram = np.abs(sgram) + 0.00000000000001  # this prevents taking the log of 0\n",
    "    log_sgram = 20 * np.log10(sgram)\n",
    " \n",
    "    # plot the spectrum \n",
    "    if display: \n",
    "        plt.figure()\n",
    "        x_coord = np.tile(times,(len(freqs),1)) \n",
    "        x_coord = np.fliplr(x_coord)\n",
    "        y_coord = np.tile(freqs,(len(times),1))\n",
    "        y_coord = np.rot90(y_coord,3)\n",
    "        \n",
    "        plt.pcolor(x_coord,y_coord,log_sgram)\n",
    "        # set the limits of the plot to the limits of the data\n",
    "        plt.axis([x_coord.min(), x_coord.max(), y_coord.min(), y_coord.max()])\n",
    "\n",
    "        plt.title('Magnitude Spectrogram in dB')\n",
    "        plt.xlabel('time in seconds')\n",
    "        plt.ylabel('frequency in Hz')\n",
    "        cbar = plt.colorbar()\n",
    "\n",
    "    return np.rot90(log_sgram,1), times, freqs\n",
    "#--------------------------------------\n",
    "# A couple of helper functions to make mels into frequencies and vice versa\n",
    "\n",
    "def mel2freq(mel):\n",
    "    freq = 700.0 * (-1.0 + 10.0**(mel/2595.0))\n",
    "    return freq\n",
    "\n",
    "def freq2mel(freq):\n",
    "    x = 1.0 + freq/700.0\n",
    "    mel = 2595 * np.log10(x)\n",
    "    return mel\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Exactly what it says it is...\n",
    "\n",
    "def make_triangular_filters(cfreqs, freqs ):\n",
    "    \"\"\"\n",
    "    Computes a set of \"filters\" to apply to an STFT to change its frequency scaling.\n",
    "    This takes an array of center frequencies (cfreqs) and a set of frequencies of \n",
    "    analysis (freqs) as input, both coded in Hz.  It will return a 2-D numpy array \n",
    "    filters[c][f], where c is the index number of a center frequency in cfreqs and \n",
    "    f is the index to a frequency in freqs and the value in filters[c][f] is a number \n",
    "    between 0 and 1 that represents how sensitive this filter is to that frequency.  \n",
    "    Filters are triangular, with a linear decrease in sensitivity from the center \n",
    "    frequency (sensitivity = 1) down to a sensitivity of 0 by the center frequency \n",
    "    of each adjacent freqency.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    cfreqs: a 1-D python array of center frequencies measured in Hz\n",
    "    freqs:   a 1-D python array of frequencies for whom we need to calculate the sensitivity\n",
    "             of each filter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filters: a 2-D numpy array, where c is the index number of a center frequency in \n",
    "            cfreqs and f is the index to a frequency in freqs and the value in filters[c][f] \n",
    "            is a number between 0 and 1 that represents how sensitive this filter is to \n",
    "            that frequency.\n",
    "    \"\"\"\n",
    "     \n",
    "    assert min(freqs)<=min(cfreqs) and max(freqs)>=max(cfreqs), \"cfreqs range must be within freqs range.\"\n",
    "\n",
    "    filters = np.zeros((len(cfreqs), len(freqs)))\n",
    "    \n",
    "    for c in range(0,len(cfreqs)):\n",
    "        if c==0: \n",
    "            # if we're on the first cfreq, use min(freq) for the low\n",
    "            low_freq = min(freqs)\n",
    "            high_freq = cfreqs[c+1]\n",
    "        elif c==(len(cfreqs)-1):\n",
    "            # if we're on the last cfreq, use max(freq) for the high\n",
    "            low_freq = cfreqs[c-1]\n",
    "            high_freq = max(freqs)\n",
    "        else:\n",
    "            low_freq = cfreqs[c-1]\n",
    "            high_freq = cfreqs[c+1]\n",
    "  \n",
    "        for f in range(len(freqs)):\n",
    "            \n",
    "            if (freqs[f]> cfreqs[c]):\n",
    "                x = freqs[f]\n",
    "                b = -high_freq / (cfreqs[c]-high_freq) # offset\n",
    "                a = 1./(cfreqs[c]-high_freq) # slope\n",
    "                filters[c][f] = max([a*x+b,0.])\n",
    "            elif (freqs[f]< cfreqs[c]):\n",
    "                x = freqs[f]\n",
    "                b = -low_freq / (cfreqs[c]-low_freq) # offset\n",
    "                a = 1./(cfreqs[c]-low_freq) # slope\n",
    "                filters[c][f] = max([a*x+b,0.])\n",
    "            else:\n",
    "                filters[c][f] = 1.0\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (2 points) Make a log spectrogram function that lets you use either log2 spacing or mel frequency spacing..  \n",
    "\n",
    "$m = 2595log_{10}(1+\\frac{f}{700})$\n",
    "\n",
    "(1) Douglas O'Shaughnessy (1987). Speech communication: human and machine. Addison-Wesley. p. 150. ISBN 978-0-201-16520-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_spectrogram(signal, window_size, hop_size, sample_rate, num_channels, channel_spacing = 'mel', display = True ):\n",
    "    \"\"\"\n",
    "    Builds a mel-frequency spectrogram. Computes the short term fourier transform of a 1-D numpy array \n",
    "    (the signal). It then creates a magnitude spectrogram of the signal and then remaps the energy num_channels\n",
    "    frequency bins, spaced evenly in the log (or mel) frequency domain between the top key of the piano (4186 Hz) and \n",
    "    the bottom key of the piano (27.5 Hz). The vertical dimension is the index number of \n",
    "    the frequency channel. The horizontal dimension is time(in seconds), brightness corresponds to \n",
    "    amplitude (in dB). Only frequencies up to the top of the piano are displayed.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    signal: The 1-d (complex or real) numpy array containing the signal\n",
    "    window_size: an integer scalar specifying the number of samples in a window\n",
    "    hop_size: an integer specifying the number of samples between the start of adjacent windows\n",
    "    sample_rate: an integer giving the sample rate of the input signal, in Hz\n",
    "    num_channels: how many frequency bins to put the energy into\n",
    "    channel_spacing: if 'mel', then space evenly in the mel range. Else, evenly in log2(freq)\n",
    "            *NOTE* if you pick 88 channels and log2 spacing, this places a bin at each piano key\n",
    "    display: a bool. If set to True, it plots the spectrogram. Else it does not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    an output tuple with 3 items\n",
    "    \n",
    "    lgram: a 2-D numpy array of real-valued numbers that contains the magnitude spectrogram.\n",
    "           lgram[t,f] is the magnitude at time t and frequency f. This only contains values\n",
    "           up to the pitch note of the piano (4186 Hz)\n",
    "    times: a 1-D numpy array of non-negative real-values that gives the times,  \n",
    "           times[t] gives the start time of the tth window in seconds\n",
    "    freqs: a 1-D numpy array  of non-negative real values. freqs[f] gives the fth\n",
    "           frequency bin in Hz. These are spaced evenly in the log2 of the frequency in Hz,\n",
    "           with 12 to an octave. \n",
    "           \n",
    "    Calling Example\n",
    "    ---------------\n",
    "    lgram,times,freqs = log_spectrogram(signal, window_size, hop_size, sample_rate, num_channels )\n",
    "\n",
    "    \"\"\" \n",
    "    # your code goes here\n",
    "    top_piano_note =  4186\n",
    "    bottom_piano_note = 27.5\n",
    "    \n",
    "    \n",
    "    N = signal.shape[0]\n",
    "    total_time = N / sample_rate\n",
    "    num_windows = np.floor( (N - window_size) / hop_size) + 1\n",
    "    times = np.arange(num_windows) * hop_size/(1.0 * sample_rate)\n",
    "\n",
    "    X = stft(signal, window_size, hop_size)\n",
    "    \n",
    "    # chop off everything above the nyquist rate & rotate for display    \n",
    "    X = np.transpose(X)\n",
    "    sgram = np.array(X[0: window_size/2 + 1])\n",
    "\n",
    "    # figure out what my frequencies of analysis are..\n",
    "    analysis_f0 = sample_rate * 1.0 /window_size\n",
    "    freqs = np.arange(len(sgram)) * analysis_f0\n",
    "\n",
    "    \n",
    "    if channel_spacing == 'mel':\n",
    "        top_mel = freq2mel(top_piano_note)\n",
    "        bottom_mel = freq2mel(bottom_piano_note)\n",
    "        mels = np.linspace(bottom_mel, top_mel, num_channels)\n",
    "        cfreqs= mel2freq(mels)\n",
    "    else:\n",
    "        cfreqs = bottom_piano_note * np.logspace(0, 87.0/12, num_channels, base=2)\n",
    "    \n",
    "    \n",
    "    filters = make_triangular_filters(cfreqs, freqs )        \n",
    "    \n",
    "    \n",
    "    #turn the complex values into magnitudes and\n",
    "    sgram = np.abs(sgram) + 0.00000000000001  # this prevents taking the log of 0\n",
    "\n",
    "    # apply filter banks to magnitude spectrogram\n",
    "    sgram = np.dot(filters, sgram)\n",
    "    \n",
    "    #  putit into a log scale     \n",
    "    lgram = 20 * np.log10(sgram)\n",
    "\n",
    "    # plot the spectrum \n",
    "    if display: \n",
    "        plt.figure()\n",
    "        y=range(len(cfreqs)+1)\n",
    "        x=range(len(times))\n",
    "        T, F = np.meshgrid(times, y)\n",
    "        plt.pcolormesh(T, F, lgram)\n",
    "\n",
    "        ytick = np.array(cfreqs,dtype='f2')\n",
    "        xtick = np.array(times, dtype='f2')\n",
    "        plt.yticks(np.arange(0.5,len(y)),ytick)\n",
    "        #plt.xticks(x,xtick)\n",
    "        plt.xlim(0,times[-1])\n",
    "        plt.ylim(0, y[-1])\n",
    "        if channel_spacing == 'mel':\n",
    "            plt.title('Mel Spectrogram in dB')\n",
    "        else:\n",
    "            plt.title('Log Spectrogram in dB')\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "        plt.xlabel(\"Times (s)\")\n",
    "        cbar = plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    return np.transpose(lgram), times, cfreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you should probably write something to test your code here.\n",
    "# fs=40000.\n",
    "# t=np.arange(4000)/fs\n",
    "# signal= np.sin(2*np.pi*110*t) + np.sin(2*np.pi*2200*t)\n",
    "signal, fs = librosa.load('piano.wav')\n",
    "\n",
    "window_size=1024\n",
    "hop_size=512\n",
    "sample_rate=fs\n",
    "num_channels = 10\n",
    "\n",
    "sgram,times,freqs = spectrogram(signal, window_size, hop_size, sample_rate )\n",
    "#print sgram.shape\n",
    "P,T,F=log_spectrogram(signal, window_size, hop_size, sample_rate, num_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ( 2 points) Make a chromagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chromagram(signal, window_size, hop_size, sample_rate, bins_per_pitchclass = 10, display = True):\n",
    "    \"\"\"\n",
    "    Computes a SIMPLE chromagram of the signal and plots it on the screen. By SIMPLE chromagram,\n",
    "    I mean that each chroma bin only gets energy from frequencies that fall within the width of the bin.\n",
    "    Another way of putting that is: Only energy around powers of 2 of the same frequencies share a chroma bin.\n",
    "    Another way of putting that is no energy from odd harmonics placed in a bin. \n",
    "    \n",
    "    Here, the vertical dimension is chroma from 0 to 1 (AKA pitch class put on a scale from 0 to 1), \n",
    "    the horizontal dimension is time (in seconds), brightness corresponds to amplitude (in dB).\n",
    "    \n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    signal: The 1-d (complex or real) numpy array containing the signal\n",
    "    window_size: an integer scalar specifying the number of samples in a window\n",
    "    hop_size: an integer specifying the number of samples between the start of adjacent windows\n",
    "    sample_rate: an integer giving the sample rate of the input signal, in Hz\n",
    "    bins_per_pitchclass: a positive integer specifying how many bins each pitchclass will contain\n",
    "    display: bool. if set to True, it displays the chromagram on the screen\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    an output tuple with 3 items\n",
    "    \n",
    "    cgram:  a 2-D numpy array of real-valued numbers that contains the magnitude chromagram\n",
    "           cgram[t,c] is the magnitude at time t and chroma c. This only contains values\n",
    "           up to the nyquist frequency\n",
    "    times: a 1-D numpy array of non-negative real-values that gives the times,  \n",
    "           times[t] gives the start time of the tth window in seconds\n",
    "    chroma: a 1-D numpy array  of non-negative real values in the range 0 to 1. chroma[c] \n",
    "            gives the cth chroma bin \n",
    "  \n",
    "    Calling Example\n",
    "    ---------------\n",
    "    cgram,times,chroma = chromagram(signal, window_size, hop_size, sample_rate, bins_per_pitchclass)\n",
    "        \n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    num_pitchclass = 12\n",
    "    #notes = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#'];\n",
    "    \n",
    "    \n",
    "    # times\n",
    "    N = signal.shape[0]\n",
    "    total_time = N / sample_rate\n",
    "    num_windows = np.floor( (N - window_size) / hop_size) + 1\n",
    "    times = np.arange(num_windows) * hop_size/(1.0 * sample_rate)\n",
    "\n",
    "    X = stft(signal, window_size, hop_size)\n",
    "    \n",
    "    # chop off everything above the nyquist rate & rotate for display    \n",
    "    X = np.transpose(X)\n",
    "    sgram = np.array(X[0: window_size/2 + 1])\n",
    "\n",
    "\n",
    "    # figure out what my frequencies of analysis are..\n",
    "    analysis_f0 = sample_rate * 1.0 /window_size\n",
    "    freqs = np.arange(len(sgram)) * analysis_f0 + 0.00000000000001\n",
    "    \n",
    "    # chroma\n",
    "    num_bins = bins_per_pitchclass * num_pitchclass\n",
    "    chroma = np.arange(num_bins) * 1.0 / num_bins\n",
    "    \n",
    "    #print freqs\n",
    "    chromas = np.log2(freqs) - np.floor(np.log2(freqs))\n",
    "    #print chromas\n",
    "    chroma_indices = np.floor(np.multiply(chromas, num_bins))\n",
    "    #print chroma_indices\n",
    "    \n",
    "    mapping = [0]* num_bins\n",
    "    for i in range(num_bins):\n",
    "        mapping_i = np.zeros(len(freqs))\n",
    "        mapping_i[np.where(chroma_indices == i)] = 1\n",
    "        mapping[i] = mapping_i\n",
    "   \n",
    "    sgram = np.dot(mapping, sgram)\n",
    "    #turn the complex values into magnitudes and\n",
    "    sgram = np.abs(sgram) + 0.00000000000001  # this prevents taking the log of 0\n",
    "    \n",
    "    #print sgram.shape\n",
    "    \n",
    "    #  putit into a log scale     \n",
    "    cgram = 20 * np.log10(sgram)\n",
    "    \n",
    "    \n",
    "    # plot the spectrum \n",
    "    if display: \n",
    "        plt.figure()\n",
    "        y=range(len(chroma)+1)\n",
    "        T, F = np.meshgrid(times, y)\n",
    "        plt.pcolormesh(T, F, cgram)\n",
    "\n",
    "        plt.yticks(np.arange(bins_per_pitchclass / 2.0, num_bins, bins_per_pitchclass),range(1,13))\n",
    "        #plt.xticks(x,xtick)\n",
    "        plt.xlim(0,times[-1])\n",
    "        plt.ylim(0, y[-1])\n",
    "        plt.title('Chromagram')\n",
    "        plt.ylabel(\"Pitch Class\")\n",
    "        plt.xlabel(\"Times (s)\")\n",
    "        cbar = plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    return np.transpose(cgram),times,chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do yourself a favor and write something to test your chromagram here.\n",
    "\n",
    "\n",
    "signal, fs = librosa.load('piano-chrom.wav')\n",
    "\n",
    "window_size=2048\n",
    "hop_size=1024\n",
    "sample_rate=fs\n",
    "bins_per_pitchclass = 2\n",
    "\n",
    "#sgram,times,freqs = spectrogram(signal, window_size, hop_size, sample_rate )\n",
    "cgram,times,chroma = chromagram(signal, window_size, hop_size, sample_rate, bins_per_pitchclass)\n",
    "\n",
    "Audio(signal,rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (2 point) Make a mel-frequency cepstrogram.  In lecture we mentioned the plain (not mel frequency) cepstrum of a time-domain signal $s$ can be calculated as follows:\n",
    "\n",
    "$ cepstrum = ifft(log|fft(s)|)$\n",
    "\n",
    "#### For audio processing, we typically don't use the FFT, but the mel-spectrogram. This means we've removed the frequencies above the Nyquist rate and therefore, things can get a little bit weird when taking the inverse FFT. A very common approach to dealing with this weirdness is to use an alternate transform called the Discrete Cosine Transform (DCT). You don't have to understand the details for this assignment , but you can find out more about cesptrums in the paper reading coming up in the 2nd half of the class.  Anyhow, now the cepstrum looks more like this:\n",
    "\n",
    "1. Get the spectrum S = fft(s)\n",
    "2. make a mel-spaced magnitude spectrum M from S, discarding frequencies above Nyquist\n",
    "3. make M' = log(M)\n",
    "4. do DCT(M') \n",
    "\n",
    "#### This is the approach we'd like you to take in making a mel-frequency cepstrogram. \n",
    "\n",
    "#### You can find a DCT implementation to use here:\n",
    "http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mf_cepstrogram(signal, window_size, hop_size, sample_rate, num_channels, display = True ):\n",
    "    \"\"\"\n",
    "    Builds a mel-frequency cepstrogram. Computes the short term fourier transform of a 1-D numpy array \n",
    "    (the signal). It then creates a magnitude spectrogram of the signal and then remaps the energy num_channels\n",
    "    frequency bins, spaced evenly in the mel frequency domain between the top key of the piano (4186 Hz) and \n",
    "    the bottom key of the piano (27.5 Hz). It then takes the DCT of the log of each magnitude spectrum. The \n",
    "    vertical dimension is the index number of the cepstral coefficient. The horizontal dimension is time(in seconds), \n",
    "    brightness corresponds to magnitude.\n",
    "    \n",
    "    Input Parameters\n",
    "    ----------------\n",
    "    signal: The 1-d (complex or real) numpy array containing the signal\n",
    "    window_size: an integer scalar specifying the number of samples in a window\n",
    "    hop_size: an integer specifying the number of samples between the start of adjacent windows\n",
    "    sample_rate: an integer giving the sample rate of the input signal, in Hz\n",
    "    num_channels: how many mel-spaced frequency bins to put the energy into. \n",
    "                This becomes the number of cepstral coefficients.\n",
    "    display: bool. If set to True, it plots the spectrogram. Else it does not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    an output tuple with 3 items\n",
    "    \n",
    "    mfccgram: a 2-D numpy array of real-valued numbers that contains the mel frequency cepstrogram.\n",
    "           mfccgram[t,c] is the magnitude at time t and cepstral coefficeint c. \n",
    "    times: a 1-D numpy array of non-negative real-values that gives the times,  \n",
    "           times[t] gives the start time of the tth window in seconds\n",
    "           \n",
    "    Calling Example\n",
    "    ---------------\n",
    "    mfccgram,times = mf_cepstrogram(signal, window_size, hop_size, sample_rate, num_channels)\n",
    "\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    top_piano_note =  4186\n",
    "    bottom_piano_note = 27.5\n",
    "    \n",
    "    # figure out the length of time\n",
    "    N = signal.shape[0]\n",
    "    total_time = N / sample_rate\n",
    "    num_windows = np.floor( (N - window_size) / hop_size) + 1\n",
    "    times = np.arange(num_windows) * hop_size/(1.0 * sample_rate)\n",
    "    \n",
    "    # get STFT spectrogram\n",
    "    X = stft(signal, window_size, hop_size)\n",
    "    num_hops = len(X)\n",
    "    \n",
    "    # chop off everything above the nyquist rate\n",
    "    X = np.transpose(X)\n",
    "    sgram = np.array(X[0: window_size/2 + 1])\n",
    "    \n",
    "    #turn the complex values into magnitudes\n",
    "    sgram = np.abs(sgram) + 0.00000000000001  # this prevents taking the log of 0\n",
    "\n",
    "    # figure out what my frequencies of analysis are..\n",
    "    analysis_f0 = sample_rate * 1.0 /window_size\n",
    "    freqs = np.arange(len(sgram)) * analysis_f0\n",
    "\n",
    "    # produce mel filter banks\n",
    "    top_mel = freq2mel(top_piano_note)\n",
    "    bottom_mel = freq2mel(bottom_piano_note)\n",
    "    mels = np.linspace(bottom_mel, top_mel, num_channels)\n",
    "    cfreqs= mel2freq(mels)  \n",
    "    filters = make_triangular_filters(cfreqs, freqs )        \n",
    "\n",
    "    # apply filter banks to magnitude spectrogram\n",
    "    sgram = np.dot(filters, sgram)\n",
    "    \n",
    "    #  putit into a log scale     \n",
    "    lgram = np.transpose(np.log(sgram))  \n",
    "     \n",
    "    print num_windows, num_hops, len(lgram)\n",
    "    # do DCT to mel spectrum\n",
    "    mfccgram = [0]*num_hops\n",
    "    for i in range(num_hops):\n",
    "        mfccgram[i] = sp.fftpack.dct(lgram[i])\n",
    "    \n",
    "    mfccgram = np.array(mfccgram)\n",
    "    print mfccgram.shape\n",
    "    # plot the spectrum \n",
    "    if display: \n",
    "        plt.figure()\n",
    "        y=range(len(cfreqs)+1)\n",
    "        x=range(len(times))\n",
    "        T, F = np.meshgrid(times, y)\n",
    "        plt.pcolormesh(T, F, np.transpose(mfccgram))\n",
    "\n",
    "        xtick = np.array(times, dtype='f2')\n",
    "        plt.yticks(np.arange(0.5,len(y)), y)\n",
    "        #plt.xticks(x,xtick)\n",
    "        plt.xlim(0,times[-1])\n",
    "        plt.ylim(0, y[-1])\n",
    "        \n",
    "        plt.title('Mel Frequency Cepstrogram in dB')\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "        plt.xlabel(\"Times (s)\")\n",
    "        cbar = plt.colorbar()\n",
    "        plt.show\n",
    " \n",
    "    return mfccgram, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test case\n",
    "signal, fs = librosa.load('piano.wav')\n",
    "\n",
    "window_size=1024\n",
    "hop_size=512\n",
    "sample_rate=fs\n",
    "num_channels = 40\n",
    "\n",
    "\n",
    "sgram,times,freqs = spectrogram(signal, window_size, hop_size, sample_rate )\n",
    "#print sgram.shape\n",
    "mfccgram,times = mf_cepstrogram(signal, window_size, hop_size, sample_rate, num_channels, display=False)\n",
    "\n",
    "mfccs=librosa.feature.mfcc(signal, sr=fs, n_mels = 40, n_mfcc= 40, n_fft=1024, fmax = 4186)\n",
    "#print mfccs.shape\n",
    "\n",
    "plt.figure(figsize = (10,4))\n",
    "y1=range(mfccgram.shape[1]+1)\n",
    "x1=range(len(times))\n",
    "T1, F1 = np.meshgrid(x1, y1)\n",
    "plt.subplot(121)\n",
    "plt.pcolormesh(T1, F1, np.transpose(mfccgram))\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "y2=range(mfccs.shape[0]+1)\n",
    "x2=range(mfccs.shape[1])\n",
    "T2, F2 = np.meshgrid(x2, y2)\n",
    "plt.subplot(122)\n",
    "plt.pcolormesh(T2, F2, mfccs)\n",
    "cbar = plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with Question 4, it is OK to use any function from librosa to do your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (1 point) The recording \"balloon_response.wav\" is of a balloon pop recorded in the Garage in front of the class. Assume that the balloon pop was a perfect impulse and that the recording made was a perfect recording of the resulting sound in the room. Explain how to use \"balloon_response.wav\" to estimate the impulse response of the room and add the sound of the Garage's reverberation to the dry (no reverb) recording \"trumpet.wav.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, if the pure signal is $x(t)$ and impulse response if $h(t)$, assuming there is no noise and distortion, so recorded signal $y(t)$ as output can be produced by following:\n",
    "$$y(t) = h(t)\\ast x(t) $$\n",
    "In frequency domain, the convolution converts to muplication, thus\n",
    "$$ Y(k) = H(k)\\times X(k) $$\n",
    "Since we know the $X(k)$ and $Y(k)$, we can estimate the frequency response $H$ by\n",
    "$$ H(k) = Y(k) / X(k)$$\n",
    "Then we just need to do inverse Fourier transform on $H$ and we can get the estimation of impulse response.\n",
    "\n",
    "After we have the estimation of impulse responce, we just need to do convolution on the \"trumpet.wav\" signal and the impulse responce so that we will get a new version of \"trumpet\" signal with the Garage's reverberation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (1 points) Now explain what possible sources of error there that might mess up the approach to estimating the Garage's impulse  response described in Question 4 and how you would go about reducing or eliminating these sources of error in your impulse response estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous method is under the assumption that there is no noise and distortion, however, in real life, these are hard to avoid, so they may mess up the estimation of impluse.\n",
    "\n",
    "To reduce distortion and noise, we should use proper equipments, and try not to let them overloaded because of to high volume. Of course, stay far away from noise source \n",
    "\n",
    "What's more, if we assume that noise and distortion are uncorrelate with input signal and they are unbiased, then, we can try a lot of estimates and do avaraging so that they may be \"washed out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. (2 points)  Below is some code we have provided to estimate the impulse response of the Garage using the double sine sweep we played and recorded. Plot the frequency response function associated with this impulse response function. Plot the frequency response of the impulse response you got using the method in Question 4. Compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since we made double chirp, we know how many samples one chirp (ie one sweep) is.\n",
    "sweep_length = 2**20\n",
    "\n",
    "# load the sweep tone played out through the speaker in the Garage\n",
    "double_sweep,sr = librosa.load('double_chirp_edit.wav', 44100)\n",
    "\n",
    "# just checking my double sweep is really 2x the sweep length. \n",
    "assert(len(double_sweep)/2==sweep_length)\n",
    "\n",
    "# load the 3rd recording made of the sweep tone made in the Garage\n",
    "double_sweep_recording3,sr = librosa.load('garage_recordings/double_sweep3.wav', 44100)\n",
    "\n",
    "# show an stft of the double sweep recorded in the Garage\n",
    "plt.figure(); plt.imshow(20*np.log10(np.abs(librosa.stft(double_sweep_recording3[sweep_length:2*sweep_length]))),origin='lower',aspect='auto')\n",
    "\n",
    "# one sweep is half of the original signal.\n",
    "single_sweep = double_sweep[:sweep_length]\n",
    "\n",
    "# calculate the impulse response from the 2nd of the two sweeps (ask Mark Cartwright why we use the 2nd one)\n",
    "ds_response = np.fft.ifft(np.fft.fft(double_sweep_recording3[sweep_length:2*sweep_length]) / np.fft.fft(single_sweep)).real\n",
    "\n",
    "# shorten the length of the impulse response to be a length I've chosen. Note that it started being 2**20 and now \n",
    "# I shortened it to 2**16. Clearly you could shorten it further, if there were some need. What would that do? \n",
    "impulse_response_function_length = 2**16\n",
    "ds_response = ds_response[:impulse_response_function_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# load the recorded balloon_response\n",
    "balloon_pop_recording,sr = librosa.load('garage_recordings/balloon_pop.wav', 44100)\n",
    "\n",
    "\n",
    "bp_response = balloon_pop_recording\n",
    "\n",
    "plt.figure(1, figsize=(12,4))\n",
    "ds_spectrogram = 20*np.log10(np.abs(librosa.stft(ds_response)))\n",
    "bp_spectrogram = 20*np.log10(np.abs(librosa.stft(bp_response)))\n",
    "plt.subplot(121)\n",
    "plt.imshow(ds_spectrogram, origin='lower',aspect='auto')\n",
    "plt.title('Double Sine Impulse Responce Spectrogram')\n",
    "cbar = plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(bp_spectrogram, origin='lower',aspect='auto')\n",
    "plt.title('Balloon Pop Impulse Responce Spectrogram')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "plt.figure(2, figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "ds_freq_reponse = sp.fftpack.fft(ds_response, len(ds_response))\n",
    "plt.plot(np.abs(ds_freq_reponse))\n",
    "plt.title('Double Sine Frequency Responce')\n",
    "plt.subplot(122)\n",
    "bp_freq_reponse = sp.fftpack.fft(bp_response, len(bp_response))\n",
    "plt.plot(np.abs(bp_freq_reponse))\n",
    "plt.title('Balloon Pop Frequency Responce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. (2 points) Use the sine sweep derived impulse response to add reverb to \"trumpet.wav\". Also use the approach you described in Question 4 to add reverb to trumpet.wav. Which sounds better? Note...it is ok for this problem to trim an impulse response or trim the trumpet file a touch so that they are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# load pure trumpet signal\n",
    "trumpet,sr = librosa.load('trumpet.wav', 44100)\n",
    "\n",
    "signal_ds_reverb = sp.signal.fftconvolve(trumpet, ds_response)\n",
    "signal_bp_reverb = sp.signal.fftconvolve(trumpet, bp_response)\n",
    "\n",
    "print 'orignal'\n",
    "IPython.display.display(Audio(trumpet, rate=sr))\n",
    "print 'use ds_response'\n",
    "IPython.display.display(Audio(signal_ds_reverb, rate=sr))\n",
    "print 'use bp_response'\n",
    "IPython.display.display(Audio(signal_bp_reverb, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm..These two version seem sound alike to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. (1 point) Go on Youtube and listen to the musical piece \"I am sitting in a room\" by Alvin Lucier. Research the piece on the web. Give your reaction to the piece. Explain how it was created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this piece quite interesting and it's a really brilliant idea. \n",
    "While it is also a little scaring, especially when you stay alone in your room listening to it at midnight. LOL.\n",
    "\n",
    "Just as Lucier said in the piece, he record himself narrating a text, and then playing the recording back into the room, and re-recording it. After that, the new recording is played back again and re-recorded again, and this process is repeated.\n",
    "\n",
    "Since all rooms have characteristic resonance or formant frequencies, the effect is that certain frequencies are emphasized as they resonate in the room, until eventually the words become unintelligible, replaced by the pure resonant harmonies and tones of the room itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. (2 points) Recreate Alvin Lucier's piece by recording your voice saying the text of the piece ONCE. Then repeatedly use an impulse response function applied to the signal, duplicating the effect of Lucier's piece without having to keep re-recording in the physical world. NOTE you may have issues to deal with in terms of normalization or length of impulse response function vs length of the recording. You're going to have to figure out those out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "n_repet = 12\n",
    "\n",
    "signal,sr = librosa.load('my_recording.wav', 44100)\n",
    "bound = max(np.abs(signal))\n",
    "\n",
    "\n",
    "reverb = signal\n",
    "for i in range(n_repet):\n",
    "    reverb = librosa.util.normalize(sp.signal.fftconvolve(reverb, ds_response))\n",
    "    signal = np.append(signal,reverb)\n",
    "\n",
    "Audio(signal, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With my HW3, I attached a photo of Prof.Pardo and me at the Garage, as a proof me showing on the demo. The name of the picture is \"Picture in Friday's Demo.JPG\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
